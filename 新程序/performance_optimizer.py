#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœŸè´§æŒä»“åˆ†æç³»ç»Ÿ - æ€§èƒ½ä¼˜åŒ–æ¨¡å—
ä¸“é—¨ç”¨äºæå‡äº‘ç«¯éƒ¨ç½²æ€§èƒ½
ä½œè€…ï¼š7haoge
é‚®ç®±ï¼š953534947@qq.com
"""

import streamlit as st
import pandas as pd
import numpy as np
import os
import pickle
import hashlib
import time
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any
import concurrent.futures
from functools import wraps
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self, cache_dir: str = "cache"):
        self.cache_dir = cache_dir
        self.ensure_cache_directory()
        self.session = self.create_optimized_session()
        
    def ensure_cache_directory(self):
        """ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
    
    def create_optimized_session(self):
        """åˆ›å»ºä¼˜åŒ–çš„HTTPä¼šè¯"""
        session = requests.Session()
        
        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        # è®¾ç½®è¶…æ—¶
        session.timeout = 30
        
        return session
    
    def get_cache_key(self, func_name: str, *args, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = f"{func_name}_{str(args)}_{str(sorted(kwargs.items()))}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def get_cache_path(self, cache_key: str) -> str:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return os.path.join(self.cache_dir, f"{cache_key}.pkl")
    
    def is_cache_valid(self, cache_path: str, max_age_hours: int = 24) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not os.path.exists(cache_path):
            return False
        
        file_time = datetime.fromtimestamp(os.path.getmtime(cache_path))
        return datetime.now() - file_time < timedelta(hours=max_age_hours)
    
    def save_to_cache(self, cache_key: str, data: Any):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        try:
            cache_path = self.get_cache_path(cache_key)
            with open(cache_path, 'wb') as f:
                pickle.dump(data, f)
        except Exception as e:
            st.warning(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {str(e)}")
    
    def load_from_cache(self, cache_key: str) -> Optional[Any]:
        """ä»ç¼“å­˜åŠ è½½æ•°æ®"""
        try:
            cache_path = self.get_cache_path(cache_key)
            if self.is_cache_valid(cache_path):
                with open(cache_path, 'rb') as f:
                    return pickle.load(f)
        except Exception as e:
            st.warning(f"ç¼“å­˜åŠ è½½å¤±è´¥: {str(e)}")
        return None
    
    def clear_old_cache(self, max_age_days: int = 7):
        """æ¸…ç†æ—§ç¼“å­˜"""
        try:
            cutoff_time = datetime.now() - timedelta(days=max_age_days)
            for filename in os.listdir(self.cache_dir):
                if filename.endswith('.pkl'):
                    file_path = os.path.join(self.cache_dir, filename)
                    file_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                    if file_time < cutoff_time:
                        os.remove(file_path)
        except Exception as e:
            st.warning(f"ç¼“å­˜æ¸…ç†å¤±è´¥: {str(e)}")

# å…¨å±€ä¼˜åŒ–å™¨å®ä¾‹
optimizer = PerformanceOptimizer()

def smart_cache(max_age_hours: int = 24):
    """æ™ºèƒ½ç¼“å­˜è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = optimizer.get_cache_key(func.__name__, *args, **kwargs)
            
            # å°è¯•ä»ç¼“å­˜åŠ è½½
            cached_data = optimizer.load_from_cache(cache_key)
            if cached_data is not None:
                st.info(f"âœ… ä½¿ç”¨ç¼“å­˜æ•°æ® - {func.__name__}")
                return cached_data
            
            # æ‰§è¡Œå‡½æ•°
            st.info(f"ğŸ”„ æ­£åœ¨è·å–æ–°æ•°æ® - {func.__name__}")
            result = func(*args, **kwargs)
            
            # ä¿å­˜åˆ°ç¼“å­˜
            if result is not None:
                optimizer.save_to_cache(cache_key, result)
            
            return result
        return wrapper
    return decorator

@st.cache_data(ttl=3600)  # Streamlitç¼“å­˜1å°æ—¶
def cached_data_fetch(func_name: str, date: str, exchange: str = None):
    """ç¼“å­˜çš„æ•°æ®è·å–å‡½æ•°"""
    import akshare as ak
    
    try:
        if func_name == "get_dce_rank_table":
            return ak.get_dce_rank_table(date=date)
        elif func_name == "get_cffex_rank_table":
            return ak.get_cffex_rank_table(date=date)
        elif func_name == "get_czce_rank_table":
            return ak.get_czce_rank_table(date=date)
        elif func_name == "get_shfe_rank_table":
            return ak.get_shfe_rank_table(date=date)
        elif func_name == "futures_gfex_position_rank":
            return ak.futures_gfex_position_rank(date=date)
        elif func_name == "get_futures_daily" and exchange:
            return ak.get_futures_daily(start_date=date, end_date=date, market=exchange)
        else:
            return None
    except Exception as e:
        st.error(f"æ•°æ®è·å–å¤±è´¥ {func_name}: {str(e)}")
        return None

class FastDataManager:
    """å¿«é€Ÿæ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = data_dir
        self.optimizer = optimizer
        
        # äº¤æ˜“æ‰€é…ç½® - æŒ‰ä¼˜å…ˆçº§æ’åº
        self.exchange_config = {
            "å¤§å•†æ‰€": {
                "func_name": "get_dce_rank_table",
                "filename": "å¤§å•†æ‰€æŒä»“.xlsx",
                "priority": 1,
                "timeout": 30
            },
            "ä¸­é‡‘æ‰€": {
                "func_name": "get_cffex_rank_table", 
                "filename": "ä¸­é‡‘æ‰€æŒä»“.xlsx",
                "priority": 2,
                "timeout": 30
            },
            "éƒ‘å•†æ‰€": {
                "func_name": "get_czce_rank_table",
                "filename": "éƒ‘å•†æ‰€æŒä»“.xlsx", 
                "priority": 3,
                "timeout": 30
            },
            "ä¸ŠæœŸæ‰€": {
                "func_name": "get_shfe_rank_table",
                "filename": "ä¸ŠæœŸæ‰€æŒä»“.xlsx",
                "priority": 4,
                "timeout": 30
            },
            "å¹¿æœŸæ‰€": {
                "func_name": "futures_gfex_position_rank",
                "filename": "å¹¿æœŸæ‰€æŒä»“.xlsx",
                "priority": 5,
                "timeout": 30
            }
        }
    
    def fetch_position_data_fast(self, trade_date: str, progress_callback=None) -> bool:
        """å¿«é€Ÿè·å–æŒä»“æ•°æ® - ä½¿ç”¨å¹¶å‘å’Œç¼“å­˜"""
        success_count = 0
        total_exchanges = len(self.exchange_config)
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘è·å–æ•°æ®
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            future_to_exchange = {}
            
            for exchange_name, config in self.exchange_config.items():
                future = executor.submit(
                    self._fetch_single_exchange_data,
                    exchange_name, config, trade_date
                )
                future_to_exchange[future] = exchange_name
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for i, future in enumerate(concurrent.futures.as_completed(future_to_exchange)):
                exchange_name = future_to_exchange[future]
                
                if progress_callback:
                    progress = (i + 1) / total_exchanges * 0.6
                    progress_callback(f"å·²å®Œæˆ {exchange_name} æ•°æ®è·å–", progress)
                
                try:
                    success = future.result(timeout=60)  # 60ç§’è¶…æ—¶
                    if success:
                        success_count += 1
                except Exception as e:
                    st.warning(f"{exchange_name} æ•°æ®è·å–å¤±è´¥: {str(e)}")
                    continue
        
        if progress_callback:
            progress_callback("æŒä»“æ•°æ®è·å–å®Œæˆ", 0.6)
        
        return success_count > 0
    
    def _fetch_single_exchange_data(self, exchange_name: str, config: dict, trade_date: str) -> bool:
        """è·å–å•ä¸ªäº¤æ˜“æ‰€æ•°æ®"""
        try:
            # ä½¿ç”¨ç¼“å­˜çš„æ•°æ®è·å–å‡½æ•°
            data_dict = cached_data_fetch(config["func_name"], trade_date)
            
            if data_dict:
                # ä¿å­˜åˆ°Excel
                save_path = os.path.join(self.data_dir, config['filename'])
                with pd.ExcelWriter(save_path, engine='openpyxl') as writer:
                    for sheet_name, df in data_dict.items():
                        # æ¸…ç†sheetåç§°
                        clean_name = sheet_name[:31].replace("/", "-").replace("*", "")
                        df.to_excel(writer, sheet_name=clean_name, index=False)
                
                return True
            
        except Exception as e:
            st.warning(f"è·å–{exchange_name}æ•°æ®å¤±è´¥: {str(e)}")
            
        return False
    
    @smart_cache(max_age_hours=6)
    def fetch_price_data_fast(self, trade_date: str, progress_callback=None) -> pd.DataFrame:
        """å¿«é€Ÿè·å–æœŸè´§è¡Œæƒ…æ•°æ®"""
        price_exchanges = [
            {"market": "DCE", "name": "å¤§å•†æ‰€"},
            {"market": "CFFEX", "name": "ä¸­é‡‘æ‰€"}, 
            {"market": "CZCE", "name": "éƒ‘å•†æ‰€"},
            {"market": "SHFE", "name": "ä¸ŠæœŸæ‰€"},
        ]
        
        all_data = []
        
        # å¹¶å‘è·å–è¡Œæƒ…æ•°æ®
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            future_to_exchange = {}
            
            for exchange in price_exchanges:
                future = executor.submit(
                    cached_data_fetch,
                    "get_futures_daily",
                    trade_date,
                    exchange["market"]
                )
                future_to_exchange[future] = exchange
            
            for i, future in enumerate(concurrent.futures.as_completed(future_to_exchange)):
                exchange = future_to_exchange[future]
                
                if progress_callback:
                    progress = 0.6 + (i / len(price_exchanges)) * 0.2
                    progress_callback(f"å·²å®Œæˆ {exchange['name']} è¡Œæƒ…æ•°æ®", progress)
                
                try:
                    df = future.result(timeout=30)
                    if df is not None and not df.empty:
                        df['exchange'] = exchange["name"]
                        all_data.append(df)
                except Exception as e:
                    st.warning(f"è·å–{exchange['name']}è¡Œæƒ…æ•°æ®å¤±è´¥: {str(e)}")
                    continue
        
        if progress_callback:
            progress_callback("è¡Œæƒ…æ•°æ®è·å–å®Œæˆ", 0.8)
        
        return pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()

def optimize_streamlit_performance():
    """ä¼˜åŒ–Streamlitæ€§èƒ½"""
    # æ¸…ç†æ—§ç¼“å­˜
    optimizer.clear_old_cache()
    
    # è®¾ç½®Streamlité…ç½®
    if 'performance_optimized' not in st.session_state:
        st.session_state.performance_optimized = True
        
        # æ˜¾ç¤ºæ€§èƒ½æç¤º
        st.info("""
        ğŸš€ **æ€§èƒ½ä¼˜åŒ–å·²å¯ç”¨**
        - âœ… æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ
        - âœ… å¹¶å‘æ•°æ®è·å–  
        - âœ… ç½‘ç»œè¿æ¥ä¼˜åŒ–
        - âœ… è‡ªåŠ¨ç¼“å­˜æ¸…ç†
        
        é¦–æ¬¡è¿è¡Œå¯èƒ½è¾ƒæ…¢ï¼Œåç»­ä¼šæ˜¾è‘—åŠ é€Ÿï¼
        """)

def show_performance_metrics():
    """æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡"""
    cache_files = [f for f in os.listdir(optimizer.cache_dir) if f.endswith('.pkl')]
    cache_size = len(cache_files)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("ç¼“å­˜æ–‡ä»¶æ•°", cache_size)
    
    with col2:
        if cache_size > 0:
            latest_cache = max(
                [os.path.join(optimizer.cache_dir, f) for f in cache_files],
                key=os.path.getmtime
            )
            cache_time = datetime.fromtimestamp(os.path.getmtime(latest_cache))
            st.metric("æœ€æ–°ç¼“å­˜", cache_time.strftime("%H:%M"))
        else:
            st.metric("æœ€æ–°ç¼“å­˜", "æ— ")
    
    with col3:
        if st.button("ğŸ—‘ï¸ æ¸…ç†ç¼“å­˜"):
            optimizer.clear_old_cache(max_age_days=0)  # æ¸…ç†æ‰€æœ‰ç¼“å­˜
            st.success("ç¼“å­˜å·²æ¸…ç†")
            st.rerun() 